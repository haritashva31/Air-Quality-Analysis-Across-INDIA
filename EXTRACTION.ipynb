{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5551abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Replace these placeholders with appropriate values\n",
    "your_app_name = \"AirQualityAnalysis\"\n",
    "your_public_ip = \" \"  # Use your IPv4 address\n",
    "your_local_ip = \"localhost\"  # Use \"localhost\" if running Spark locally\n",
    "\n",
    "# Creating a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AirQualityAnalysis\") \\\n",
    "    .config(\"spark.ui.reverseProxy\", True) \\\n",
    "    .config(\"spark.ui.reverseProxyUrl\", \"http://localhost:4040\") \\\n",
    "    .config(\"spark.ui.reverseProxyAttempts\", 1) \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20d0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "from pymongo import MongoClient\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "database_name = \"hello\"\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa787dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_info_data = list(db[\"stations_info.csv\"].find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5829d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_info_schema = StructType([\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"agency\", StringType(), True),\n",
    "    StructField(\"station_location\", StringType(), True),\n",
    "    StructField(\"start_month\", StringType(), True),\n",
    "    StructField(\"start_month_num\", IntegerType(), True),\n",
    "    StructField(\"start_year\", IntegerType(), True),\n",
    "])\n",
    "df_stations_info = spark.createDataFrame(stations_info_data, schema=stations_info_schema).drop('_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644cd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f0cb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------------+------+--------------------+-----------+---------------+----------+\n",
      "|file_name|         state|             city|agency|    station_location|start_month|start_month_num|start_year|\n",
      "+---------+--------------+-----------------+------+--------------------+-----------+---------------+----------+\n",
      "|    AP001|Andhra Pradesh|         Tirupati| APPCB| Tirumala, Tirupati |       July|              7|      2016|\n",
      "|    AP002|Andhra Pradesh|       Vijayawada| APPCB|PWD Grounds, Vija...|        May|              5|      2017|\n",
      "|    AP003|Andhra Pradesh|    Visakhapatnam| APPCB|GVM Corporation, ...|       July|              7|      2017|\n",
      "|    AP004|Andhra Pradesh|Rajamahendravaram| APPCB|Anand Kala Kshetr...|  September|              9|      2017|\n",
      "|    AP005|Andhra Pradesh|        Amaravati| APPCB|Secretariat, Amar...|   November|             11|      2017|\n",
      "|    AP006|Andhra Pradesh|        Anantapur| APPCB|Gulzarpet, Ananta...|     August|              8|      2022|\n",
      "|    AP007|Andhra Pradesh|         Chittoor| APPCB|Gangineni Cheruvu...|   November|             11|      2022|\n",
      "|    AP008|Andhra Pradesh|         Tirupati| APPCB|Vaikuntapuram, Ti...|   November|             11|      2022|\n",
      "|    AP009|Andhra Pradesh|       Vijayawada| APPCB| Kanuru, Vijayawada |    January|              1|      2023|\n",
      "|    AP010|Andhra Pradesh|           Kadapa| APPCB|Yerramukkapalli, ...|    January|              1|      2023|\n",
      "+---------+--------------+-----------------+------+--------------------+-----------+---------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_stations_info.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16545d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_for_prefix(prefix):\n",
    "    prefix_collections = [f\"{prefix}{i:03d}\" for i in range(1, 41)]\n",
    "\n",
    "#Fetching the data from each collection and storing it in a list\n",
    "    prefix_data_list = []\n",
    "    for collection_name in prefix_collections:\n",
    "        prefix_data = list(db[collection_name].find())\n",
    "        prefix_data_list.extend(prefix_data)\n",
    "\n",
    "#Defining the schema\n",
    "    prefix_data_schema = StructType([\n",
    "    StructField(\"From Date\", TimestampType(), True),\n",
    "    StructField(\"To Date\", TimestampType(), True),\n",
    "    StructField(\"PM2.5 (ug/m3)\", DoubleType(), True),\n",
    "    StructField(\"PM10 (ug/m3)\", DoubleType(), True),\n",
    "    StructField(\"NO (ug/m3)\", DoubleType(), True),\n",
    "    StructField(\"NO2 (ug/m3)\", DoubleType(), True),\n",
    "    StructField(\"NOx (ppb)\", DoubleType(), True),\n",
    "    StructField(\"SO2 (ug/m3)\", DoubleType(), True),\n",
    "    StructField(\"CO (mg/m3)\", DoubleType(), True),\n",
    "    StructField(\"Ozone (ug/m3)\", DoubleType(), True),\n",
    "    StructField(\"Benzene (ug/m3)\", DoubleType(), True),\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "])\n",
    "\n",
    "#Converting string dates to TimestampType\n",
    "    for entry in prefix_data_list:\n",
    "        entry['From Date'] = datetime.strptime(entry['From Date'], '%Y-%m-%d %H:%M:%S')\n",
    "        entry['To Date'] = datetime.strptime(entry['To Date'], '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    prefix_df = spark.createDataFrame(prefix_data_list, schema=prefix_data_schema)\n",
    "    prefix_df = prefix_df.drop('_id')\n",
    "\n",
    "    return prefix_df\n",
    "\n",
    "dl_df = create_dataframe_for_prefix(\"DL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8e92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 16:35:56 WARN TaskSetManager: Stage 1 contains a task of very large size (7489 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+----------+-------------+------------------+---------+\n",
      "|          From Date|            To Date|    PM2.5 (ug/m3)|     PM10 (ug/m3)|NO (ug/m3)|NO2 (ug/m3)|NOx (ppb)|SO2 (ug/m3)|CO (mg/m3)|Ozone (ug/m3)|   Benzene (ug/m3)|file_name|\n",
      "+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+----------+-------------+------------------+---------+\n",
      "|2010-01-01 00:00:00|2010-01-01 01:00:00|83.72420430274217|73.29047204423317|     21.02|       41.0|    38.75|       4.27|      4.43|          3.0|2.2437990905332783|    DL001|\n",
      "|2010-01-01 01:00:00|2010-01-01 02:00:00|83.72420430274217|73.29047204423317|      9.12|       29.5|    23.25|       4.55|      3.69|          3.5|2.2437990905332783|    DL001|\n",
      "|2010-01-01 02:00:00|2010-01-01 03:00:00|83.72420430274217|73.29047204423317|     10.48|      27.25|    23.25|       4.62|      3.68|          3.5|2.2437990905332783|    DL001|\n",
      "|2010-01-01 03:00:00|2010-01-01 04:00:00|83.72420430274217|73.29047204423317|       6.5|      24.25|    18.25|       4.52|      3.11|          4.0|2.2437990905332783|    DL001|\n",
      "|2010-01-01 04:00:00|2010-01-01 05:00:00|83.72420430274217|73.29047204423317|      5.75|       21.5|    16.25|        4.7|       2.8|          4.0|2.2437990905332783|    DL001|\n",
      "|2010-01-01 05:00:00|2010-01-01 06:00:00|83.72420430274217|73.29047204423317|     11.35|       23.0|    21.25|        4.7|      2.26|          4.0|2.2437990905332783|    DL001|\n",
      "|2010-01-01 06:00:00|2010-01-01 07:00:00|83.72420430274217|73.29047204423317|      12.6|       24.0|     23.0|       4.65|      2.99|         4.25|2.2437990905332783|    DL001|\n",
      "|2010-01-01 07:00:00|2010-01-01 08:00:00|83.72420430274217|73.29047204423317|     19.65|      32.25|    33.25|       4.55|      4.19|          4.0|2.2437990905332783|    DL001|\n",
      "|2010-01-01 08:00:00|2010-01-01 09:00:00|83.72420430274217|73.29047204423317|      24.4|      39.25|    40.75|        4.6|       4.2|          4.0|2.2437990905332783|    DL001|\n",
      "|2010-01-01 09:00:00|2010-01-01 10:00:00|83.72420430274217|73.29047204423317|     13.78|       34.5|     29.5|        5.0|      3.33|         4.75|2.2437990905332783|    DL001|\n",
      "+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+----------+-------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 16:36:00 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 1 (TID 1): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dl_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc85180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- From Date: timestamp (nullable = true)\n",
      " |-- To Date: timestamp (nullable = true)\n",
      " |-- PM2.5 (ug/m3): double (nullable = true)\n",
      " |-- PM10 (ug/m3): double (nullable = true)\n",
      " |-- NO (ug/m3): double (nullable = true)\n",
      " |-- NO2 (ug/m3): double (nullable = true)\n",
      " |-- NOx (ppb): double (nullable = true)\n",
      " |-- SO2 (ug/m3): double (nullable = true)\n",
      " |-- CO (mg/m3): double (nullable = true)\n",
      " |-- Ozone (ug/m3): double (nullable = true)\n",
      " |-- Benzene (ug/m3): double (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dl_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4766970",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka_df = create_dataframe_for_prefix(\"KA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d4d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- From Date: timestamp (nullable = true)\n",
      " |-- To Date: timestamp (nullable = true)\n",
      " |-- PM2.5 (ug/m3): double (nullable = true)\n",
      " |-- PM10 (ug/m3): double (nullable = true)\n",
      " |-- NO (ug/m3): double (nullable = true)\n",
      " |-- NO2 (ug/m3): double (nullable = true)\n",
      " |-- NOx (ppb): double (nullable = true)\n",
      " |-- SO2 (ug/m3): double (nullable = true)\n",
      " |-- CO (mg/m3): double (nullable = true)\n",
      " |-- Ozone (ug/m3): double (nullable = true)\n",
      " |-- Benzene (ug/m3): double (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "215d1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_df = create_dataframe_for_prefix(\"MH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b75ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df = create_dataframe_for_prefix(\"UP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0fd103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_df = create_dataframe_for_prefix(\"TN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4aab25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- From Date: timestamp (nullable = true)\n",
      " |-- To Date: timestamp (nullable = true)\n",
      " |-- PM2.5 (ug/m3): double (nullable = true)\n",
      " |-- PM10 (ug/m3): double (nullable = true)\n",
      " |-- NO (ug/m3): double (nullable = true)\n",
      " |-- NO2 (ug/m3): double (nullable = true)\n",
      " |-- NOx (ppb): double (nullable = true)\n",
      " |-- SO2 (ug/m3): double (nullable = true)\n",
      " |-- CO (mg/m3): double (nullable = true)\n",
      " |-- Ozone (ug/m3): double (nullable = true)\n",
      " |-- Benzene (ug/m3): double (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "up_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76cfcf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 16:53:31 WARN TaskSetManager: Stage 2 contains a task of very large size (3748 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+----------+-------------+------------------+---------+\n",
      "|          From Date|            To Date|    PM2.5 (ug/m3)|     PM10 (ug/m3)|NO (ug/m3)|NO2 (ug/m3)|NOx (ppb)|SO2 (ug/m3)|CO (mg/m3)|Ozone (ug/m3)|   Benzene (ug/m3)|file_name|\n",
      "+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+----------+-------------+------------------+---------+\n",
      "|2010-01-01 00:00:00|2010-01-01 01:00:00|47.98693658729114|68.10118941403422|     77.97|     167.54|   245.51|       4.71|      2.25|         2.71|0.9528246674307944|    TN001|\n",
      "|2010-01-01 01:00:00|2010-01-01 02:00:00|47.98693658729114|68.10118941403422|      64.1|     139.83|   203.94|       6.45|      2.26|          2.7|0.9528246674307944|    TN001|\n",
      "|2010-01-01 02:00:00|2010-01-01 03:00:00|47.98693658729114|68.10118941403422|     44.99|     100.53|   145.53|       8.12|       0.8|         2.31|0.9528246674307944|    TN001|\n",
      "|2010-01-01 03:00:00|2010-01-01 04:00:00|47.98693658729114|68.10118941403422|     42.99|      92.17|   135.16|       6.59|      0.27|         2.26|0.9528246674307944|    TN001|\n",
      "|2010-01-01 04:00:00|2010-01-01 05:00:00|47.98693658729114|68.10118941403422|     32.31|      69.98|    102.3|       4.66|      1.05|         2.07|0.9528246674307944|    TN001|\n",
      "|2010-01-01 05:00:00|2010-01-01 06:00:00|47.98693658729114|68.10118941403422|     44.07|      89.94|   134.01|       7.06|      1.38|         2.28|0.9528246674307944|    TN001|\n",
      "|2010-01-01 06:00:00|2010-01-01 07:00:00|47.98693658729114|68.10118941403422|     44.37|      88.43|    132.8|       9.01|      1.17|         2.59|0.9528246674307944|    TN001|\n",
      "|2010-01-01 07:00:00|2010-01-01 08:00:00|47.98693658729114|68.10118941403422|     22.07|      52.61|    74.69|       6.68|      1.47|         2.98|0.9528246674307944|    TN001|\n",
      "|2010-01-01 08:00:00|2010-01-01 09:00:00|47.98693658729114|68.10118941403422|     15.19|      44.65|    59.85|       9.49|      1.45|         7.23|0.9528246674307944|    TN001|\n",
      "|2010-01-01 09:00:00|2010-01-01 10:00:00|47.98693658729114|68.10118941403422|      8.83|      30.63|    39.45|      12.69|      1.04|        19.74|0.9528246674307944|    TN001|\n",
      "+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+----------+-------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 16:53:35 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 2): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tn_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "212f4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_keep = [\"DL001\", \"DL002\", \"DL003\", \"DL004\", \"DL005\", \"DL006\", \"KA001\", \"KA002\", \"KA003\", \"MH001\", \"MH002\", \"MH003\", \"TN001\", \"TN002\", \"TN003\", \"UP001\", \"UP002\"]\n",
    "\n",
    "filtered_df_stations_info = df_stations_info.filter(col('file_name').isin(files_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40193d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------+--------------------+-----------+---------------+----------+\n",
      "|file_name|        state|       city|agency|    station_location|start_month|start_month_num|start_year|\n",
      "+---------+-------------+-----------+------+--------------------+-----------+---------------+----------+\n",
      "|    DL001|        Delhi|      Delhi|  CPCB|         ITO, Delhi |    January|              1|      2010|\n",
      "|    DL002|        Delhi|      Delhi|  CPCB|    Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL003|        Delhi|      Delhi|  CPCB|    Sirifort, Delhi |    January|              1|      2010|\n",
      "|    DL004|        Delhi|      Delhi|  CPCB| NSIT Dwarka, Delhi |    January|              1|      2010|\n",
      "|    DL005|        Delhi|      Delhi|  CPCB|IHBAS, Dilshad Ga...|    January|              1|      2010|\n",
      "|    DL006|        Delhi|      Delhi|  CPCB|         DTU, Delhi |    January|              1|      2010|\n",
      "|    KA001|    Karnataka|  Bengaluru|  CPCB|BTM Layout, Benga...|    January|              1|      2010|\n",
      "|    KA002|    Karnataka|  Bengaluru|  CPCB|BWSSB Kadabesanah...|    January|              1|      2010|\n",
      "|    KA003|    Karnataka|  Bengaluru|  CPCB|  Peenya, Bengaluru |    January|              1|      2010|\n",
      "|    MH001|  Maharashtra|       Pune|  MPCB|   Karve Road, Pune |    January|              1|      2010|\n",
      "|    MH002|  Maharashtra|     Mumbai|  MPCB|     Bandra, Mumbai |    January|              1|      2010|\n",
      "|    MH003|  Maharashtra|Navi Mumbai|  MPCB|Airoli, Navi Mumbai |    January|              1|      2010|\n",
      "|    TN001|   Tamil Nadu|    Chennai|  CPCB|Alandur Bus Depot...|    January|              1|      2010|\n",
      "|    TN002|   Tamil Nadu|    Chennai|  CPCB|    Manali, Chennai |    January|              1|      2010|\n",
      "|    TN003|   Tamil Nadu|    Chennai|  CPCB|Velachery Res. Ar...|    January|              1|      2010|\n",
      "|    UP001|Uttar Pradesh|    Lucknow|  CPCB|Talkatora Distric...|    January|              1|      2010|\n",
      "|    UP002|Uttar Pradesh|    Lucknow|  CPCB|Kendriya Vidyalay...|    January|              1|      2010|\n",
      "+---------+-------------+-----------+------+--------------------+-----------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df_stations_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d51260c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Example for dl_df\n",
    "prefix_dl_df = \"DL\"  # Extracted from the DataFrame name\n",
    "filtered_dl_info = filtered_df_stations_info.filter(col('file_name').startswith(prefix_dl_df))\n",
    "\n",
    "# Join dl_df with filtered_dl_info\n",
    "dl_df_combined = dl_df.join(filtered_dl_info, on='file_name', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e3533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 16:53:44 WARN TaskSetManager: Stage 6 contains a task of very large size (7489 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 6:==========>       (6 + 4) / 10][Stage 7:>                 (0 + 6) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+-----------------+-------------+---------------+-----+-----+------+----------------+-----------+---------------+----------+\n",
      "|file_name|          From Date|            To Date|    PM2.5 (ug/m3)|     PM10 (ug/m3)|NO (ug/m3)|NO2 (ug/m3)|NOx (ppb)|SO2 (ug/m3)|       CO (mg/m3)|Ozone (ug/m3)|Benzene (ug/m3)|state| city|agency|station_location|start_month|start_month_num|start_year|\n",
      "+---------+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+-----------------+-------------+---------------+-----+-----+------+----------------+-----------+---------------+----------+\n",
      "|    DL002|2010-01-01 00:00:00|2010-01-01 01:00:00|59.41818649235221|46.85813662670525|     68.69|     132.78|   201.47|        3.2|             1.21|         5.76|          54.66|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 01:00:00|2010-01-01 02:00:00|59.41818649235221|46.85813662670525|     12.42|      65.33|    77.75|       1.03|2.368121641174039|         1.87|          32.16|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 02:00:00|2010-01-01 03:00:00|59.41818649235221|46.85813662670525|      7.48|      40.04|    47.51|       4.92|2.368121641174039|         1.76|          16.63|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 03:00:00|2010-01-01 04:00:00|59.41818649235221|46.85813662670525|      4.22|      27.98|     32.2|       5.59|2.368121641174039|         1.35|          12.44|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 04:00:00|2010-01-01 05:00:00|59.41818649235221|46.85813662670525|     12.91|      32.46|    45.37|       5.57|2.368121641174039|          2.0|          11.61|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 05:00:00|2010-01-01 06:00:00|59.41818649235221|46.85813662670525|     23.52|      42.91|    66.43|       4.95|             1.53|         2.67|          16.47|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 06:00:00|2010-01-01 07:00:00|59.41818649235221|46.85813662670525|      8.96|      33.28|    42.24|       2.21|             2.15|          2.1|          13.63|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 07:00:00|2010-01-01 08:00:00|59.41818649235221|46.85813662670525|     12.16|      33.53|     45.7|       2.49|             2.97|         4.15|          15.38|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 08:00:00|2010-01-01 09:00:00|59.41818649235221|46.85813662670525|     26.27|       43.6|    69.87|        3.4|             1.69|         3.31|          16.08|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "|    DL002|2010-01-01 09:00:00|2010-01-01 10:00:00|59.41818649235221|46.85813662670525|     34.29|      87.36|   121.65|       3.66|             1.72|         3.22|          18.64|Delhi|Delhi|  CPCB|Shadipur, Delhi |    January|              1|      2010|\n",
      "+---------+-------------------+-------------------+-----------------+-----------------+----------+-----------+---------+-----------+-----------------+-------------+---------------+-----+-----+------+----------------+-----------+---------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dl_df_combined.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1be8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_tn_df = \"TN\" \n",
    "filtered_tn_info = filtered_df_stations_info.filter(col('file_name').startswith(prefix_tn_df))\n",
    "tn_df_combined = tn_df.join(filtered_tn_info, on='file_name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df52604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_up_df = \"UP\"  \n",
    "filtered_up_info = filtered_df_stations_info.filter(col('file_name').startswith(prefix_up_df))\n",
    "up_df_combined = up_df.join(filtered_up_info, on='file_name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fbd0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_mh_df = \"MH\" \n",
    "filtered_mh_info = filtered_df_stations_info.filter(col('file_name').startswith(prefix_mh_df))\n",
    "mh_df_combined = mh_df.join(filtered_mh_info, on='file_name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d35c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_ka_df = \"KA\"  \n",
    "filtered_ka_info = filtered_df_stations_info.filter(col('file_name').startswith(prefix_ka_df))\n",
    "ka_df_combined = ka_df.join(filtered_ka_info, on='file_name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22eaec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 17:02:35 WARN TaskSetManager: Stage 15 contains a task of very large size (7489 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/11/30 17:02:42 WARN TaskSetManager: Stage 20 contains a task of very large size (3748 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/11/30 17:02:43 WARN TaskSetManager: Stage 25 contains a task of very large size (2427 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/11/30 17:02:43 WARN TaskSetManager: Stage 30 contains a task of very large size (3748 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/11/30 17:02:44 WARN TaskSetManager: Stage 35 contains a task of very large size (3748 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrames to CSV locally\n",
    "dl_df_combined.write.csv(\"file:///Users/meetsmacbook/Downloads/spark csvs/dl_combined.csv\", header=True, mode=\"overwrite\")\n",
    "tn_df_combined.write.csv(\"file:///Users/meetsmacbook/Downloads/spark csvs/tn_combined.csv\", header=True, mode=\"overwrite\")\n",
    "up_df_combined.write.csv(\"file:///Users/meetsmacbook/Downloads/spark csvs/up_combined.csv\", header=True, mode=\"overwrite\")\n",
    "ka_df_combined.write.csv(\"file:///Users/meetsmacbook/Downloads/spark csvs/ka_combined.csv\", header=True, mode=\"overwrite\")\n",
    "mh_df_combined.write.csv(\"file:///Users/meetsmacbook/Downloads/spark csvs/mh_combined.csv\", header=True, mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
